# Golang
**Зачем** - Шедулер нужен, чтобы управлять горутинами на потоках ОС, что позволяет на одном потоке "иметь несколько горутин". 
**Цели**: -использовать наименьшее кол-во тредов ОС наиболее эффективно; -поддержка параллелизма: на n ядер - n горутин; -поддержка конкурентности. 
**Когда** - запускается на операциях, которые могут или должны заафектить исполнение горутины: -создание/блокировка горутины -system call (сам поток тоже блочится)

**Основные принципы шедулера**
-   Все **готовые** горутины нуждающиеся в планировании трекаются в **очередях** (**runqueue** heap-allocated FIFO), число которых равно кол-ву ядер проца. Треды берут горутины из runqueue**.**
-   Треды **дополнительно поднимаются** для новых горутин в случае, если **все остальные заняты**.
-   **Thread parking**: если же остается тред **без горутин**, то мы его **паркуем (sleep mode)** - он не потребляет ресурсы процессора.
-   **Tрекаем все idle треды** для переиспользования, если придут новые горутины.
-   **Work stealing:** если же при запуске нового треда его локальная очеред пуста, то он может **воровать** горутины (обычно половину) из чужой рандомной очереди. Этот принцип помогает балансить загрузку между тредами.
-   **Handoff (передача):** если все треды заняты и, допустим, одна из горутин создала новую г-ну, а потом данный тред (T2 с очередью q2) блокируется, то **монитор** смотрит на наличие **припаркованных** тредов для передачи очереди. Если не находит, то создает **новый** тред (T3) и **передает** ей свою очередь (q2). Этот принцип предостерегает от "голодания" ресурсов при заблокированных тредах.
-   Многозадачность шедулера является **"кооперативно вытесняющей"** одновременно. Есть бекграунд тред "**sysmon**" для выявления горутин с долгим жизненным циклом (>10ms), который **unschedule-ит** их, если это возможно. Шедулер **не будет возвращать их в per-core runqueues**, он положит горутину в **global runqueue (=lower priority queue)**
-   **Thread spinning**
    -   Треды без работы проставиют в поиске перед паркингом, они чекают глобальную очередь, poll-ит нетворк, пытается выполнять таски gc, а также воровать горутины из очередей. 
    -   Это сжирает циклы проца, но зато максимально использует доступный параллелизм.
-   **P's и runqueues**
    -   Per-core очереди хранятся в структуре **"p"** лежащей в куче.
    -   Также она хранит ресурсы, которые нужны треду для запуска горутин, к примеру кеш.

**Многозадачность в Го** "_неявная кооперативность/вытеснение": -_ Горутина уступает место другим при обращении к вводу/выводу, каналам, вызовам ОС - Может уступить место при вызове любой функции - runtime.Gosched() - Рассматривается внедренгие вытесняющей многозадачности.

**Основные принципы планировщика:** - Очередь FIFO - порядок выполнения горутин - Необходимый минимум тредов - Захват чужой работы - тред после выполнения горутины будет пытаться забрать невыполняемые горутины себе. - "Неинвазивность" - не прерывает нативную работу горутин.

**Ограничения планировщика:** - FIFO - нет приоритезации для горутин - Отсутствие гарантий времени выполнения - Горутины могут перемещаться между тредами, что влияет на кеши процессора и работу с оперативкой

В планировщике Go есть две разные очереди выполнения: глобальная очередь выполнения (GRQ) и локальная очередь выполнения (LRQ). Каждому P присваивается LRQ, который управляет горутинами, назначенными для выполнения в контексте P. Эти горутины по очереди включаются и выключаются из контекста M, назначенного для этого P. GRQ предназначен для горутин, которые не были назначены для P. Существует процесс, чтобы переместить горутины из GRQ в LRQ, который мы обсудим позже. На схеме так же стоит отметить, что есть очередь горутин у P, а есть **исполняемая одна** горутина. P могут скакать по разным M - переключаться. **M** - поток ОС, **Р** - аппаратный поток, **G** - горутина _P, M - это не что-то связанное с железом или ОС напрямую, это абстракции использующиеся в Го (см. исходники)_

![[../Files/Pasted image 20220928220458.png]]

**Кооперативный планировщик** Планировщик ОС является вытесняющим планировщиком. По сути, это означает, что вы не можете предсказать, что планировщик собирается делать в любой момент времени. Ядро принимает решения и все недетерминировано. Приложения, работающие поверх операционной системы, не контролируют то, что происходит внутри ядра с планированием, если они не используют примитивы синхронизации, такие как атомарные инструкции и вызовы мьютекса. Планировщик Go является частью среды исполнения Go, а среда исполнения Go встроена в ваше приложение. Это означает, что планировщик Go работает в пользовательском пространстве над ядром. Текущая реализация планировщика Go является не вытесняющим, а **взаимодействующим планировщиком**. Быть кооперативным планировщиком означает, что планировщику нужны четко определенные события в пространстве пользователя, которые происходят в безопасных точках кода для принятия решений по планированию. Что хорошо в **кооперативном планировщике Go**, так это то, что он выглядит и чувствует себя упреждающим. **Вы не можете предсказать, что собирается делать планировщик Go**. Это связано с тем, что принятие решений для этого планировщика зависит не от разработчиков, а от времени выполнения Go. Важно думать о планировщике Go как об упреждающем планировщике, и, поскольку планировщик недетерминирован, это не слишком сложно.

**Состояния Горутин** - **_Ожидание_**: горутина остановлена и ждет чего-то, чтобы продолжить. Это может происходить по таким причинам, как ожидание операционной системы (системные вызовы) или синхронизация вызовов (атомарные и мьютексные операции). Эти типы задержек являются основной причиной плохой производительности. - _**Готовность**_: горутина хочет получить время, чтобы выполнить назначенные инструкции. Если у вас много горутин, которым нужно время, то горутине придется ждать дольше, чтобы получить время. Кроме того, индивидуальное количество времени, которое получает любая горутина, сокращено, поскольку больше горутин конкурируют за время. - **Выполнение**: горутина была помещена в M и выполняет свои инструкции.

**Очень условный код создания горутины и один раунд шедулера:**
```go
func newproc1(fn...) {
    _g_ := getg() // текущая горутина
    _p_ := _g_.m.p.ptr() // текущий процессор
    ...
    newg := gfget(_p_) // новая горутина
    newg.startpc = fn // должна выполняться с пришедшей функцией
    newg.return_to = goexit // магия... после завершения переходим в ф-ю goexit
    runqput(_p_, newg) // добавляем горутину в очередь процессора
}
func schedule() {
    g := get_gc_worker() // проверяем не нужно ли выполнить сборку мусора
    if g == nil {
        g = runqget() // берем текущую горутину из очереди треда
    }
    if g == nil {
        // если нет горутины, то пытаемся украсть горутину из чужих тредов или ждем
        g = steal_or_wait()
    }
    execute(g) // выполняем горутину
}
```

# Golang
Сборка мусора выполняется конкурентно.

```go
func printStats(mem runtime.MemStats) {  
   runtime.ReadMemStats(&mem)  
   fmt.Println("mem.Alloc:", mem.Alloc)  
   fmt.Println("mem.TotalAlloc:", mem.TotalAlloc)  
   fmt.Println("mem.HeapAlloc:", mem.HeapAlloc)  
   fmt.Println("mem.NumGC:", mem.NumGC)  
   fmt.Println("-----")  
}  
  
func main() {  
   var mem runtime.MemStats  
   printStats(mem)  
   /*  
      mem.Alloc: 184424      
      mem.TotalAlloc: 184424      
      mem.HeapAlloc: 184424      
      mem.NumGC: 0  
   */   
   for i := 0; i < 10; i++ {  
      s := make([]byte, 50000000)  
      if s == nil {  
         fmt.Println("Operation failed!")  
      }  
   }  
   printStats(mem)  
   /*  
      mem.Alloc: 184424      
      mem.TotalAlloc: 184424      
      mem.HeapAlloc: 184424      
      mem.NumGC: 0   
  */   
  for i := 0; i < 10; i++ {  
      s := make([]byte, 100000000)  
      if s == nil {  
         fmt.Println("Operation failed!")  
      }  
      time.Sleep(5 * time.Second)  
   }  
   printStats(mem)  
   /*  
      mem.Alloc: 173144      
      mem.TotalAlloc: 1500316576      
      mem.HeapAlloc: 173144      
      mem.NumGC: 20  
   */}
```

Если перед любой командой go run поставить `GODEBUG=gctrace=1`, то Go выводит аналитические данные о работе сборщика мусора. Данные представлены в такой форме:
```
GODEBUG=gctrace=1 go run gColl.go
gc 4 @0.025s 0%: 0.002+0.065+0.018 ms clock,
0.021+0.040/0.057/0.003+0.14 ms cpu, 47->47->0 MB, 48 MB goal, 8 P
gc 17 @30.103s 0%: 0.004+0.080+0.019 ms clock,
0.033+0/0.076/0.071+0.15 ms cpu, 95->95->0 MB, 96 MB goal, 8 P
```

Здесь приводится информация о размерах кучи - 47->47->0 - первое число - размер кучи перед запуском GC, второе число - размер кучи, когда GC завершает работу, после число - актуальное значение кучи

## Трехцветный алгоритм
Главный принцип - разделение объектов, находящихся в куче, на 3 группы в соответствии с цветом который определяет алгоритм. Цвета:
1. Черный - объекты не имеют указателей ни на один объект белого цвета
2. Белый - может иметь указатель на объект черного цвета
3. Серый - может иметь указатель на объект белого цвета

Объекты белого цвета являются претендентами на удаление. Ни один объект не может перейти непосредственно из черного множества в белое. Ни один объект из черного множества не может напрямую указывать на объект из белого множества.

Работа алгоритма:
Когда начинается сборка мусора все объекты становятся белыми. Сборщик мусора перебирает все корневые объекты и окрашивает их в серый цвет. Корневые объекты — это объекты, к которым приложение может обращаться напрямую, включая глобальные переменные и другие элементы, находящиеся в стеке.

После этого сборщик мусора выбирает серый объект, помечает его черным и проверяет, есть ли у него указатели на другие объекты из белого множества. Если проверка обнаружит, что у данного объекта есть один или несколько указателей на белые объекты, алгоритм поменяет цвет этих белых объектов на серый. Процесс продолжается до тех пор, пока не будут перебраны все объекты серого множества. Затем объекты белого множества считаются недостижимыми, и занимаемая ими память может использоваться повторно.

![[../Files/Pasted image 20220928220858.png]]